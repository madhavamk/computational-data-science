{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/madhavamk/computational-data-science/blob/master/MiniProjects/M5_MP2_NB_phi_2_Open_Source_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced Certification Program in Computational Data Science\n",
        "## A programme by IISc and TalentSprint\n",
        "### Mini-Project: Open Source Retrieval Augmented Generation (RAG)"
      ],
      "metadata": {
        "id": "i5gB6iG94kzV"
      },
      "id": "i5gB6iG94kzV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem Statement\n",
        "\n",
        "Retrieval Q and A Integrated with LLM"
      ],
      "metadata": {
        "id": "I0FW5b1NQ_Wu"
      },
      "id": "I0FW5b1NQ_Wu"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learning Objectives\n",
        "\n",
        "At the end of the experiment you will be able to :\n",
        "\n",
        "1. Run Phi-2, Microsoft's small language model (SLM), using two methods:\n",
        "   - Direct Inference using HuggingFace\n",
        "   - Retrieval Augmented Generation (RAG) using Llama-index\n",
        "2. Know the basic working of Llama Index VectorStore\n",
        "3. Implement the Hugging Face embedding\n",
        "4. Implement a simple FAISS-based vector store for efficient similarity search of high-dimensional data.\n",
        "5. Create RetrievalQA chain along with prompt template\n",
        "6. Compare the **effectiveness of Phi-2 & Zephyr-7b-beta model** by means of Cosine Similarity.\n",
        "7. Compare the **effectiveness of 5 different Hugging Face embeddings** by computing and analyzing the cosine similarity between the embedded vectors of queries and results from Zephyr-7b-beta model, to understand the differences in semantic similarity and performance.\n"
      ],
      "metadata": {
        "id": "1jylUQmEIH-y"
      },
      "id": "1jylUQmEIH-y"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Information\n",
        "\n",
        "Retrieval Augmented Generation (RAG) combines the advanced text-generation capabilities of GPT and other large language models with information retrieval functions to provide precise and contextually relevant information. This innovative approach improves language models' ability to understand and process user queries by integrating the latest and most relevant data. As RAG continues to evolve, its growing applications are set to revolutionize AI efficiency and utility.\n",
        "\n",
        "##Retrieval-Augmented Generation (RAG) Process\n",
        "###  **Feeding LLMs with Accurate Information**:\n",
        "\n",
        "- Instead of directly querying the language model, relevant data is first retrieved from a well-maintained knowledge library.\n",
        "\n",
        "\n",
        "###**Retrieval Before Generation**:\n",
        "\n",
        "- Accurate data is retrieved using vector embeddings (numerical representations of the data).\n",
        "- These embeddings help match the query with relevant documents in a vector database.\n",
        "\n",
        "\n",
        "###**Context for Generation**:\n",
        "\n",
        "- Once the requested document or information is found, the retrieved context is used by the model to generate the answer.\n",
        "\n",
        "\n",
        "###**Reduces Hallucinations**:\n",
        "\n",
        "- This approach lowers the risk of hallucinations, where the model generates inaccurate or false information.\n",
        "\n",
        "\n",
        "###**No Need for Retraining**:\n",
        "\n",
        "- The knowledge base can be updated without retraining the model, making the system adaptable without incurring high costs.\n",
        "\n",
        "\n",
        "###**Cost-Effective Model Updates**:\n",
        "\n",
        "- By using a retriever system, models can be updated dynamically without the expense of a full model retraining process.\n",
        "\n",
        "<br><br>\n",
        "<center>\n",
        "<img src=\" https://cdn.exec.talentsprint.com/static/cds/RAG_Image.jpg\" width= 600 px/>\n",
        "</center>\n",
        "<br><br>\n",
        "\n",
        "RAG brings together four key components:\n",
        "\n",
        "- **Embedding model**: This is where documents are turned into vectors, or numerical representations, which make it easier for the system to manage and compare large amounts of text data.\n",
        "- **Retriever**: Think of this as the search engine within RAG. It uses the embedding model to process a question and fetch the most relevant document vectors that match the query.\n",
        "- **Reranker (optional)**: This component takes things a step further by evaluating the retrieved documents to determine how relevant they are to the question at hand, providing a relevance score for each one.\n",
        "- **Language model**: Finally, this part of the system takes the top documents provided by the retriever or reranker, along with the original question, and crafts a precise answer.\n",
        "To know more about the RAG, refer [here](https://www.superannotate.com/blog/rag-explained)."
      ],
      "metadata": {
        "id": "Vi22cd57RXms"
      },
      "id": "Vi22cd57RXms"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In this notebook, we'll explore how to run Phi-2, Microsoft's small language model (SLM), using two methods:\n",
        "- Direct Inference using HuggingFace\n",
        "- Retrieval Augmented Generation (RAG) using Llama-index\n",
        "\n",
        "Phi-2 is an SLM with 2.7 billion parameters and trained on 1.4T tokens."
      ],
      "metadata": {
        "id": "lt_QVg04RLot"
      },
      "id": "lt_QVg04RLot"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDCe4r5fwDtD"
      },
      "source": [
        "## Benefits of Small Models\n",
        "- Fast fine-tuning\n",
        "- Can be run locally\n",
        "- Requires less computational resources\n",
        "\n",
        "### **Note: This notebook has to necessarily run on GPU.**"
      ],
      "id": "xDCe4r5fwDtD"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grading = 10 Points"
      ],
      "metadata": {
        "id": "dIu0egmxkROS"
      },
      "id": "dIu0egmxkROS"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXWCjoVEwDtD"
      },
      "source": [
        "## Install Required Packages\n",
        "Install necessary libraries for running Phi-2 on Google Colab."
      ],
      "id": "oXWCjoVEwDtD"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "H1QOUdb2wDtE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54b03681-5fec-45b9-84a6-9e5af30e27dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m255.2/255.2 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m404.4/404.4 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m607.0/607.0 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m66.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.8/273.8 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.7/94.7 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m77.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m104.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.7/149.7 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.0/64.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m420.0/420.0 kB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.3/73.3 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m100.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m425.7/425.7 kB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.1/164.1 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip -qq install langchain torch transformers sentencepiece accelerate bitsandbytes einops sentence-transformers\n",
        "!pip -qq install langchain_community\n",
        "!pip -qq install langchain_huggingface\n",
        "!pip -qq install huggingface_hub\n",
        "!pip -qq install chromadb"
      ],
      "id": "H1QOUdb2wDtE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing necessary packages"
      ],
      "metadata": {
        "id": "5sUNWYCUWIMW"
      },
      "id": "5sUNWYCUWIMW"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from getpass import getpass\n",
        "from langchain import hub\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from transformers import pipeline\n",
        "from langchain_community.llms import HuggingFaceEndpoint\n",
        "from langchain_community.chat_models.huggingface import ChatHuggingFace\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceHubEmbeddings\n",
        "from langchain.prompts import PromptTemplate\n",
        "from transformers import pipeline\n",
        "\n",
        "from langchain.llms import HuggingFaceHub\n",
        "from langchain import LLMChain\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "#from llama_index.embeddings import HuggingFaceEmbedding\n",
        "\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.document_loaders.csv_loader import CSVLoader\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "13z90ZwCWCMg"
      },
      "id": "13z90ZwCWCMg",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Phase-I:** Comparison between Microsoft Phi-2 and Hugging Face Zephyr-7b-beta without Retrieval Augmented Generation (RAG)"
      ],
      "metadata": {
        "id": "Wm6u9sf8dpcU"
      },
      "id": "Wm6u9sf8dpcU"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zg2qL17QwDtF"
      },
      "source": [
        "## 1.1 Load the Phi-2 Model and Tokenizer to integrate with Langchain using HuggingFace Pipeline\n",
        "\n",
        "<br><br>\n",
        "<center>\n",
        "<img src=\" https://cdn.exec.talentsprint.com/static/cds/content/Phi_2_without_RAG-1.png\" width= 600 px/>\n",
        "</center>\n",
        "<br><br>\n",
        "\n",
        "**Exercise-1:** Load Phi-2 model and tokenizer from Huggingface and create a pipeline for text generation. Then integrate the Phi-2 model with Langchain for better prompt handling. **(0.5 point)**"
      ],
      "id": "Zg2qL17QwDtF"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8BDZfrVYwDtF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "67d1742d6c1d4907bdcc16a8f1a7fcb2",
            "ce6d7f8482c249949fc81c29092b947a",
            "1739b8125a284b5db23f6e5bba352846",
            "bd3b14096fb548c9be24136ae97b06a7",
            "a98a47af5156429389d3bc9944d78ab0",
            "bdd494cfafe6456eb6705a860a65a33e",
            "cc6c72d799a74449953843af428cc39b",
            "c0d50216ea8e497cbd18f664e3b7ff07",
            "7ade35d08ebc46a08d3f671e4faab597",
            "057c6b7f0ae048dbb8d8b5f6483a8c7e",
            "a711137a125f41ada1c7c91b58095c89"
          ]
        },
        "outputId": "b54a73f3-c8e6-4405-cdb7-d43cbc46eb5d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "67d1742d6c1d4907bdcc16a8f1a7fcb2"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, pipeline, AutoModelForCausalLM\n",
        "from langchain import HuggingFacePipeline\n",
        "import transformers\n",
        "import torch\n",
        "\n",
        "# Get model's tokenizer using AutoTokenizer.from_pretrained()\n",
        "\n",
        "# YOUR CODE HERE\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\")\n",
        "# Load the 'microsoft/phi-2' model for causal language modeling. Use AutoModelForCausalLM.from_pretrained()\n",
        "# with torch_dtype='auto' and device_map='auto'\n",
        "\n",
        "# YOUR CODE HERE\n",
        "model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\", torch_dtype='auto', device_map='auto')\n",
        "\n",
        "\n",
        "# Create a text-generation pipeline using the transformers library with a specific model and tokenizer,\n",
        "# Adjust parameters such as device_map='auto', token limits = 256, and temperature = 0.5\n",
        "\n",
        "# YOUR CODE HERE\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device_map='auto',\n",
        "    max_length=256,\n",
        "    temperature=0.5\n",
        ")"
      ],
      "id": "8BDZfrVYwDtF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcMLV9b0wDtG"
      },
      "source": [
        "Integrating the Phi-2 model with Langchain for better prompt handling."
      ],
      "id": "xcMLV9b0wDtG"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "rAlwk0iXwDtG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "991062b9-0422-476f-a183-a0d29a264dab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-2e0690f5aeed>:32: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
            "  phi2_HFP_llm_chain = LLMChain(llm=HuggingFacePipeline(pipeline=pipe), prompt=prompt)\n",
            "<ipython-input-5-2e0690f5aeed>:32: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  phi2_HFP_llm_chain = LLMChain(llm=HuggingFacePipeline(pipeline=pipe), prompt=prompt)\n"
          ]
        }
      ],
      "source": [
        "from langchain import HuggingFacePipeline\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains.llm import LLMChain\n",
        "\n",
        "# Creating a Text-Generation Pipeline Using Hugging Face Transformers\n",
        "# configure the pad token in a Hugging Face model when using a pipeline for text generation\n",
        "\n",
        "# YOUR CODE HERE\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device_map='auto',\n",
        "    max_length=256,\n",
        "    temperature=0.5,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "# Define a prompt template\n",
        "task_template = '''\n",
        "You are a friendly chatbot assistant that gives structured output.\n",
        "Your role is to arrange the given task in this structure.\n",
        "### instruction:\n",
        "{instruction}\n",
        "Output:\n",
        "'''\n",
        "\n",
        "# Creating a Task Prompt Template and LLM Chain Using phi-2 Model. Store it in variable 'phi2_HFP_llm_chain'\n",
        "\n",
        "# YOUR CODE HERE\n",
        "prompt = PromptTemplate(input_variables=[\"instruction\"], template=task_template)\n",
        "phi2_HFP_llm_chain = LLMChain(llm=HuggingFacePipeline(pipeline=pipe), prompt=prompt)"
      ],
      "id": "rAlwk0iXwDtG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGh5g1fawDtG"
      },
      "source": [
        "## 1.3 Querying the Phi-2 Model\n",
        "**Exercise-2:** Now let's query the model with a prompt. For example, let's ask the model to 'Give an overview of Computational Data Science PG Level certificaion course'. From the response, extract the 'text' field and save it in a variable 'phi_2_extracted_output'. **(0.5 point)**"
      ],
      "id": "PGh5g1fawDtG"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "rz6R7S4nwDtH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09ac0bdf-9c28-4836-a7e3-6016db39aab2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-00e63cf14d5b>:8: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  response = phi2_HFP_llm_chain.run(question)\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "You are a friendly chatbot assistant that gives structured output.\n",
            "Your role is to arrange the given task in this structure.\n",
            "### instruction:\n",
            "Give an overview of Computational Data Science PG Level certificaion course\n",
            "Output:\n",
            "The Computational Data Science PG Level Certification course is designed to provide an overview of the field of computational data science.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Example query\n",
        "question = 'Give an overview of Computational Data Science PG Level certificaion course'\n",
        "\n",
        "# Invoke the language model chain 'phi2_HFP_llm_chain' to generate a response to the above question and\n",
        "# print the response.\n",
        "\n",
        "# YOUR CODE HERE\n",
        "response = phi2_HFP_llm_chain.run(question)\n",
        "print(response)"
      ],
      "id": "rz6R7S4nwDtH"
    },
    {
      "cell_type": "code",
      "source": [
        "# In the above code cell, you have already got Simulated response from the model.\n",
        "# The response has 2 keys ('instruction' and 'text').\n",
        "# Extract the 'text' field from the response.\n",
        "\n",
        "# YOUR CODE HERE\n",
        "response_text = response[response.find('instruction:'):]\n",
        "\n",
        "# Parse the text to get the output part only\n",
        "# Assuming the output starts after the keyword \"Output:\"\n",
        "# (Find the index after \"Output:\")\n",
        "\n",
        "# YOUR CODE HERE\n",
        "output_start_index = response_text.find(\"Output:\") + len(\"Output:\")\n",
        "\n",
        "# (Extract the output part and strip extra whitespace &\n",
        "                   # then store it in a variable 'phi_2_extracted_output')\n",
        "\n",
        "# YOUR CODE HERE\n",
        "phi_2_extracted_output = response_text[output_start_index:].strip()\n",
        "\n",
        "# Print the extracted output variable 'phi_2_extracted_output'\n",
        "\n",
        "# YOUR CODE HERE\n",
        "print(phi_2_extracted_output)"
      ],
      "metadata": {
        "id": "cRvj0kTLcH5X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78ecafb2-7097-43e6-ac40-45aecf4f7fce"
      },
      "id": "cRvj0kTLcH5X",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Computational Data Science PG Level Certification course is designed to provide an overview of the field of computational data science.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4 Using the HuggingFace API Key"
      ],
      "metadata": {
        "id": "07y4h_tAWrq2"
      },
      "id": "07y4h_tAWrq2"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "h_api_key = userdata.get('HF_TOKEN')"
      ],
      "metadata": {
        "id": "wooEmPPIW3Bh"
      },
      "id": "wooEmPPIW3Bh",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set your HuggingFace API key\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = h_api_key"
      ],
      "metadata": {
        "id": "b5lOv1YiW6R4"
      },
      "id": "b5lOv1YiW6R4",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.5 Initializing HuggingFaceEndpoint with [**HuggingFaceH4/zephyr-7b-beta**](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta) model\n",
        "\n",
        "<br><br>\n",
        "<center>\n",
        "<img src=\" https://cdn.exec.talentsprint.com/static/cds/content/zephyr_without_RAG-2.png\" width= 600 px/>\n",
        "</center>\n",
        "<br><br>\n",
        "\n"
      ],
      "metadata": {
        "id": "oWm_gy3JXIwP"
      },
      "id": "oWm_gy3JXIwP"
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize HuggingFaceEndpoint with your endpoint URL\n",
        "endpoint_url = \"https://huggingface.co/HuggingFaceH4/zephyr-7b-beta\"\n",
        "\n",
        "# Initialize the model name \"HuggingFaceH4/zephyr-7b-beta\" in a variable model_name\n",
        "model_name = \"HuggingFaceH4/zephyr-7b-beta\""
      ],
      "metadata": {
        "id": "0wJjL6m4XNI5"
      },
      "id": "0wJjL6m4XNI5",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.6 Creating the LLM using zephyr-7b-beta\n",
        "\n",
        "**Exercise-3:** Create an LLM using HuggingFaceEndpoint. **(0.5 point)**"
      ],
      "metadata": {
        "id": "aEAwW1nsXVur"
      },
      "id": "aEAwW1nsXVur"
    },
    {
      "cell_type": "code",
      "source": [
        "# Import HuggingFace model abstraction class from langchain\n",
        "from langchain_huggingface import HuggingFaceEndpoint"
      ],
      "metadata": {
        "id": "-dFhLKSQXbR_"
      },
      "id": "-dFhLKSQXbR_",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an LLM using HuggingFaceEndpoint.\n",
        "# Configure HuggingFaceEndpoint for text generation with the specified parameters such as max_new_tokens = 512,\n",
        "# top_k = 30, temperature = 0.1 and repetition_penalty = 1.03. Store the created llm in\n",
        "# a variable 'zephyr_7b_beta_HFE_llm'\n",
        "\n",
        "# YOUR CODE HERE\n",
        "zephyr_7b_beta_HFE_llm = HuggingFaceEndpoint(\n",
        "    # endpoint_url=endpoint_url,\n",
        "    repo_id=\"HuggingFaceH4/zephyr-7b-beta\",\n",
        "    task=\"text-generation\",\n",
        "    max_new_tokens = 512,\n",
        "    top_k = 30,\n",
        "    temperature = 0.1,\n",
        "    repetition_penalty = 1.03,\n",
        ")\n",
        ""
      ],
      "metadata": {
        "id": "gtKzNe-sXfaI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3eedeca-6265-4050-e2db-29056758e63d"
      },
      "id": "gtKzNe-sXfaI",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.7 Querying the HuggingFace zephyr-7b-beta Model\n",
        "Now let's query the model with a prompt. For example, let's ask the model to give an overview of the Computational Data Science PG Level certification course."
      ],
      "metadata": {
        "id": "iWcA098TX1ZP"
      },
      "id": "iWcA098TX1ZP"
    },
    {
      "cell_type": "code",
      "source": [
        "# Query the model with a prompt and ask the model to \"Give an overview of Computational Data Science PG Level certificaion course\"\n",
        "\n",
        "zephyr_7b_beta_response = zephyr_7b_beta_HFE_llm.invoke(\"Give an overview of Computational Data Science PG Level certificaion course\")\n",
        "print(zephyr_7b_beta_response)"
      ],
      "metadata": {
        "id": "e_7uZ_X6Xk4J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52052bd7-ec5f-4529-dafa-b0c9ddf04470"
      },
      "id": "e_7uZ_X6Xk4J",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " offered by IIIT-Bangalore and its benefits.\n",
            "\n",
            "IIIT-Bangalore, a premier institute in the field of Information Technology, has launched a new course in Computational Data Science at the postgraduate level. This course is designed to provide students with a deep understanding of data science concepts and techniques using computational methods. In this article, we will provide an overview of the course and its benefits.\n",
            "\n",
            "Course Overview:\n",
            "\n",
            "The Computational Data Science PG Level certification course offered by IIIT-Bangalore is a 12-month program that covers various aspects of data science, including data preprocessing, feature engineering, machine learning algorithms, deep learning, and big data analytics. The course is delivered through a combination of online lectures, interactive sessions, and hands-on projects.\n",
            "\n",
            "The course curriculum is divided into four modules:\n",
            "\n",
            "1. Foundations of Data Science: This module covers the basics of data science, including data cleaning, data wrangling, and data visualization using Python and R.\n",
            "\n",
            "2. Machine Learning and Deep Learning: This module covers various machine learning algorithms, such as linear regression, logistic regression, decision trees, random forests, and gradient boosting machines. It also covers deep learning concepts, such as neural networks, convolutional neural networks, and recurrent neural networks.\n",
            "\n",
            "3. Big Data Analytics: This module covers big data technologies, such as Hadoop, Spark, and NoSQL databases. It also covers distributed computing concepts, such as MapReduce and Spark Streaming.\n",
            "\n",
            "4. Capstone Project: This module involves a capstone project where students apply the concepts learned in the previous modules to solve a real-world problem.\n",
            "\n",
            "Benefits:\n",
            "\n",
            "1. Industry-relevant curriculum: The course curriculum is designed in collaboration with industry experts to ensure that students learn the latest trends and techniques in data science.\n",
            "\n",
            "2. Hands-on experience: The course includes several hands-on projects that provide students with practical experience in data science.\n",
            "\n",
            "3. Flexible learning: The course is delivered online, which allows students to learn at their own pace and from anywhere in the world.\n",
            "\n",
            "4. Expert faculty: The course is taught by experienced faculty members who have worked in the industry and have a deep understanding of data science concepts.\n",
            "\n",
            "5. Networking opportunities: The course provides students with\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.8 Comparison: Microsoft Phi-2 and Hugging Face zephyr-7b-beta model\n",
        "\n",
        "**Exercise-4:** Compare the RetrievalQA performance between Phi-2 and Hugging Face and zephyr-7b-beta model using Cosine Similarity. **(0.5 point)**\n",
        "\n",
        "- **(a)** Consider the reference Question: 'Give an overview of Computational Data Science PG Level certificaion course'. Compute Cosine Similarity.\n",
        "\n",
        "- **(b)** Consider the Benchmark_solution: 'Are you a working professional looking to build expertise in Data Science? Look no further than the PG Level Advanced Certification course in\n",
        "Data Science offered by Indian Institute of Science (IISc) in association with TalentSprint. This highly sought-after programme offers a unique 5-step learning process, including LIVE online faculty-led interactive sessions, capstone projects, mentorship, case studies, and data stories. Taught by world-class faculty from a global institution and supplemented with industry learnings, this 12-month programme is best suited for professionals who want to gain practical hands-on experience in solving real-life challenges. The programme teaches participants how to build powerful models to generate actionable insights, necessary for making data-driven decisions. With an overwhelming response, this programme has enabled 750+ professionals to build Data Science expertise. Don't miss the opportunity to gain an in-depth understanding of the mechanics of working with data and identifying insights. Enroll now and take your career to the next level with the PG Level Advanced Certification course in Computational Data Science.' Compute Cosine Similarity."
      ],
      "metadata": {
        "id": "PWXRAKAQZuXn"
      },
      "id": "PWXRAKAQZuXn"
    },
    {
      "cell_type": "code",
      "source": [
        "# (a)\n",
        "Q1 = \"Give an overview of Computational Data Science PG Level certificaion course\"\n",
        "# Instantiate the Hugging Face embeddings class and embed the query 'Q1' while reshaping the result into a 2D NumPy array\n",
        "\n",
        "# YOUR CODE HERE\n",
        "embedding = HuggingFaceHubEmbeddings()\n",
        "Q1_embedding = embedding.embed_query(Q1)\n",
        "Q1_embedding = np.array(Q1_embedding).reshape(1, -1)\n",
        "\n",
        "\n",
        "# Embed the extracted output from phi-2 ('phi_2_extracted_output') using Hugging Face embeddings and\n",
        "# reshape the result into a 2D NumPy array\n",
        "\n",
        "# YOUR CODE HERE\n",
        "phi2_embedding = embedding.embed_query(phi_2_extracted_output)\n",
        "phi2_embedding = np.array(phi2_embedding).reshape(1, -1)\n",
        "\n",
        "# Use Hugging Face embeddings to embed the response from Zephyr 7b beta ('zephyr_7b_beta_response') and\n",
        "# reshape it into a 2D NumPy array\n",
        "\n",
        "# YOUR CODE HERE\n",
        "zephyr_embedding = embedding.embed_query(zephyr_7b_beta_response)\n",
        "zephyr_embedding = np.array(zephyr_embedding).reshape(1, -1)"
      ],
      "metadata": {
        "id": "uqchYQNgaIso",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "557153aa-fb9d-4c0d-d8ab-ff02fb29158f"
      },
      "id": "uqchYQNgaIso",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-26-87740c4510df>:6: LangChainDeprecationWarning: The class `HuggingFaceHubEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEndpointEmbeddings``.\n",
            "  embedding = HuggingFaceHubEmbeddings()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute cosine similarity\n",
        "# Calculate the cosine similarity between Q1 and 'phi_2_extracted_output'\n",
        "\n",
        "# YOUR CODE HERE\n",
        "# q1_phi2_cosine = cosine_similarity(Q1_embedding, phi2_embedding)\n",
        "q1_phi2_cosine = np.dot(Q1_embedding, phi2_embedding.T) / (np.linalg.norm(Q1_embedding) * np.linalg.norm(phi2_embedding))\n",
        "print(q1_phi2_cosine)\n",
        "\n",
        "# Calculate the cosine similarity between Q1 and 'zephyr_7b_beta_response'\n",
        "\n",
        "# YOUR CODE HERE\n",
        "q1_zypher_cosine = np.dot(Q1_embedding, zephyr_embedding.T) / (np.linalg.norm(Q1_embedding) * np.linalg.norm(zephyr_embedding))\n",
        "print(q1_zypher_cosine)"
      ],
      "metadata": {
        "id": "2zrPzfkgdl6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab18cbac-f5fd-4d8c-e6f0-2451f6b419a7"
      },
      "id": "2zrPzfkgdl6f",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.86831832]]\n",
            "[[0.73686596]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (b)\n",
        "Benchmark_solution = \"\"\"Are you a working professional looking to build expertise in Data Science?\n",
        "Look no further than the PG Level Advanced Certification course in Data Science offered by Indian Institute of Science (IISc)\n",
        "in association with NSE TalentSprint. This highly sought-after programme offers a unique 5-step learning process, including\n",
        "LIVE online faculty-led interactive sessions, capstone projects, mentorship, case studies, and data stories.\n",
        "Taught by world-class faculty from a global institution and supplemented with industry learnings, this 12-month programme is best suited\n",
        "for professionals who want to gain practical hands-on experience in solving real-life challenges. The programme teaches participants\n",
        "how to build powerful models to generate actionable insights, necessary for making data-driven decisions.\n",
        "With an overwhelming response, this programme has enabled 750+ professionals to build Data Science expertise.\n",
        "Don't miss the opportunity to gain an in-depth understanding of the mechanics of working with data and identifying insights.\n",
        "Enroll now and take your career to the next level with the PG Level Advanced Certification course in Computational Data Science.\"\"\"\n",
        "\n",
        "# Embed the Benchmark Solution (BMS) using Hugging Face embeddings and reshape it into a 2D array\n",
        "\n",
        "# YOUR CODE HERE\n",
        "embedding = HuggingFaceHubEmbeddings()\n",
        "BMS_embedding = embedding.embed_query(Benchmark_solution)\n",
        "BMS_embedding = np.array(BMS_embedding).reshape(1, -1)\n",
        "\n",
        "# Embed the extracted output from phi-2 ('phi_2_extracted_output') using Hugging Face embeddings and\n",
        "# reshape the result into a 2D NumPy array\n",
        "\n",
        "# YOUR CODE HERE\n",
        "phi2_embedding = embedding.embed_query(phi_2_extracted_output)\n",
        "phi2_embedding = np.array(phi2_embedding).reshape(1, -1)\n",
        "\n",
        "# Embed the response from Zephyr 7b beta ('zephyr_7b_beta_response') and\n",
        "# reshape it into a 2D NumPy array\n",
        "\n",
        "# YOUR CODE HERE\n",
        "zephyr_embedding = embedding.embed_query(zephyr_7b_beta_response)\n",
        "zephyr_embedding = np.array(zephyr_embedding).reshape(1, -1)"
      ],
      "metadata": {
        "id": "B-c_hfSvvMDm"
      },
      "id": "B-c_hfSvvMDm",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute cosine similarity\n",
        "# Calculate and print the cosine similarity between the Benchmark Solution (BMS) and 'phi_2_extracted_output'\n",
        "\n",
        "# YOUR CODE HERE\n",
        "# BMS_phi2_cosine = cosine_similarity(BMS_embedding, phi2_embedding)\n",
        "BMS_phi2_cosine = np.dot(BMS_embedding, phi2_embedding.T) / (np.linalg.norm(BMS_embedding) * np.linalg.norm(phi2_embedding))\n",
        "print(BMS_phi2_cosine)\n",
        "\n",
        "# Calculate and print the cosine similarity between the Benchmark Solution (BMS) and 'zephyr_7b_beta_response'\n",
        "\n",
        "# YOUR CODE HERE\n",
        "# BMS_zephyr_cosine = cosine_similarity(BMS_embedding, zephyr_embedding)\n",
        "BMS_zephyr_cosine = np.dot(BMS_embedding, zephyr_embedding.T) / (np.linalg.norm(BMS_embedding) * np.linalg.norm(zephyr_embedding))\n",
        "print(BMS_zephyr_cosine)"
      ],
      "metadata": {
        "id": "EpWLckZ7wwGf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6aeb3417-94c8-44a5-a6ab-41dfe344c81b"
      },
      "id": "EpWLckZ7wwGf",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.79826978]]\n",
            "[[0.81962846]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cac1XHwwDtH"
      },
      "source": [
        "#**Phase-II:** Performing Retrieval Augmented Generation (RAG) with Microsoft Phi-2\n",
        "\n",
        "<br><br>\n",
        "<center>\n",
        "<img src=\" https://cdn.exec.talentsprint.com/static/cds/content/Phi_2_with_RAG-3.png\" width= 1200 px/>\n",
        "</center>\n",
        "<br><br>\n",
        "\n",
        "## 2.1 Retrieval Augmented Generation (RAG) with Llama-index\n",
        "\n",
        "In this section, we'll implement RAG using Llama-index to augment the retrieval from document data."
      ],
      "id": "5cac1XHwwDtH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YKZ7geLzwDtH"
      },
      "outputs": [],
      "source": [
        "!pip install -q pypdf llama-index python-dotenv"
      ],
      "id": "YKZ7geLzwDtH"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0455w-K7wDtI"
      },
      "source": [
        "## 2.2 Setup Llama-index\n",
        "Load necessary components, read documents, and set up the RAG pipeline."
      ],
      "id": "0455w-K7wDtI"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -qq install --upgrade llama-index\n",
        "!pip -qq install llama-index-embeddings-langchain\n",
        "!pip -qq install llama_index.llms.ollama\n",
        "!pip -qq install llama_index.embeddings.huggingface\n",
        "!pip -qq install llama-index-llms-langchain\n",
        "!pip -qq install faiss-gpu"
      ],
      "metadata": {
        "id": "YcNDk7YXVAyD"
      },
      "id": "YcNDk7YXVAyD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Importing necessary packages from Llama-index"
      ],
      "metadata": {
        "id": "tFKcnzQMwuLg"
      },
      "id": "tFKcnzQMwuLg"
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.indices.vector_store.base import VectorStoreIndex\n",
        "from llama_index.core import SimpleDirectoryReader\n",
        "from langchain.vectorstores import FAISS\n",
        "from llama_index.core import ServiceContext"
      ],
      "metadata": {
        "id": "3zwvTV9Sg5Xk"
      },
      "id": "3zwvTV9Sg5Xk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 2.4 Download Dataset\n",
        "#!wget -qq https://cdn.exec.talentsprint.com/static/cds/content/pca_d1.pdf\n",
        "#!wget -qq https://cdn.exec.talentsprint.com/static/cds/content/ens_d2.pdf\n",
        "!wget -qq https://cdn.exec.talentsprint.com/static/cds/content/demo_faqs.csv\n",
        "!wget -qq https://cdn.exec.talentsprint.com/static/cds/content/docs.zip\n",
        "!unzip docs.zip -d docs  # This will unzip docs.zip into a folder named 'docs'\n",
        "print(\"Dataset downloaded successfully!!\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "iIlzQjWbjxsN"
      },
      "id": "iIlzQjWbjxsN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.5 Load Data (PDF Document)"
      ],
      "metadata": {
        "id": "QSDW75lduXIS"
      },
      "id": "QSDW75lduXIS"
    },
    {
      "cell_type": "code",
      "source": [
        "# Read documents\n",
        "documents = SimpleDirectoryReader('/content/docs/docs').load_data()\n",
        "documents"
      ],
      "metadata": {
        "id": "uRWwT8Y47GQd"
      },
      "id": "uRWwT8Y47GQd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.6 Creating the Embedding Model using HuggingFaceEmbeddings **'BAAI/bge-small-en-v1.5'**\n",
        "\n",
        "**Exercise-5:** Define an embedding model using HuggingFaceEmbeddings 'BAAI/bge-small-en-v1.5'. **(0.5 point)**"
      ],
      "metadata": {
        "id": "_vb3QoueuI1n"
      },
      "id": "_vb3QoueuI1n"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the embedding model using HuggingFaceEmbeddings 'BAAI/bge-small-en-v1.5'\n",
        "\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "g02xx49Ino8s"
      },
      "id": "g02xx49Ino8s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.7 Create a Vector Store using VectorStoreIndex\n",
        "\n",
        "**Exercise-6:** Create the vector index and vector store from documents using the embedding model (used in Exercise-5). **(0.5 point)**"
      ],
      "metadata": {
        "id": "Ty7m5RcvvX3z"
      },
      "id": "Ty7m5RcvvX3z"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the vector index from documents (as loaded under section 2.5)\n",
        "# using the embedding model as achieved in Exercise-5\n",
        "\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "UzoSxoe31pz3"
      },
      "id": "UzoSxoe31pz3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the vector store from documents (as loaded under section 2.5)\n",
        "# using the embedding model (as derived in Exercise-5) and the vector index (as derived in the above code cell)\n",
        "\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "2K5HE2l4vpvM"
      },
      "id": "2K5HE2l4vpvM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.8 Create Query Engines and Test the RAG Pipeline\n",
        "\n",
        "**Exercise-7:** Create a Query Engine by using 'as_query_engine()' and then test the RAG pipeline for the Query: 'Give an overview of Computational Data Science PG Level certificaion course'. From the response, extract the text part and save it in a variable 'answer_text'. **(0.5 point)**"
      ],
      "metadata": {
        "id": "a28wIwrDwTgy"
      },
      "id": "a28wIwrDwTgy"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a query engine using the vector index and the phi-2 language model in the context of document retrieval\n",
        "# Use as_query_engine()\n",
        "\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "_fjvU0Sswbur"
      },
      "id": "_fjvU0Sswbur",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run a sample query to test the RAG pipeline."
      ],
      "metadata": {
        "id": "jTdmu21ew9ew"
      },
      "id": "jTdmu21ew9ew"
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the RAG pipeline\n",
        "# Query a document retrieval engine for the query 'Give an overview of Computational Data Science PG Level certificaion course' and\n",
        "# print the resulting response\n",
        "\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "tl8Pzu7CBGOb"
      },
      "id": "tl8Pzu7CBGOb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the text part of the above response from a result string that contains a specified prefix, such as \"Answer:\"\n",
        "# and display the answer text\n",
        "\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "2TatHqLZXLe3"
      },
      "id": "2TatHqLZXLe3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.9 RAG Performance Evaluation using Cosine Similarity\n",
        "\n",
        "**Exercise-8:** Measure the RAG performance using Cosine Similarity. **(0.5 point)**\n",
        "\n",
        "- **(a)** Consider the reference Question: 'Give an overview of Computational Data Science PG Level certificaion course'. Calculate the Cosine Similarity.\n",
        "- **(b)** Consider the Benchmark_solution [as considered in Exercise-4 (b)]. Calculate the Cosine Similarity."
      ],
      "metadata": {
        "id": "spvWWQOq4ONe"
      },
      "id": "spvWWQOq4ONe"
    },
    {
      "cell_type": "code",
      "source": [
        "# (a)\n",
        "# Generate a 2D array representation of the embeddings for the query Q1 [mentioned in Exercise-4(a)]\n",
        "# using the embedding model specified under Exercise-5\n",
        "\n",
        "# YOUR CODE HERE\n",
        "\n",
        "# Obtain a 2D array representation of the embeddings for the extracted answer text (which was achieved in Exercise-7)\n",
        "# Use the same specified embedding model mentioned in Exercise-5. Store the output in 'RAG_with_phi_2_e'\n",
        "\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "cNYOGEC94wbU"
      },
      "id": "cNYOGEC94wbU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate and print the cosine similarity between the query embedding and the RAG response embedding\n",
        "\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "Oq3aYcRd5jm-"
      },
      "id": "Oq3aYcRd5jm-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fill in the following blanks with the help of the above results [achieved in Exercise-4(a) and Exercise-8(a)]\n",
        "- Cosine Similarity between Q1 and phi_2_extracted_output: ___________________\n",
        "- Cosine Similarity between Q1 and zephyr_7b_beta_response: ___________________\n",
        "- Cosine Similarity between Q1 and RAG response: ___________________\n",
        "\n",
        "**So considering the reference query Q1, we can observe from the above value, that the Cosine Similarity is ______% by using RAG Architecture with Microsoft Phi-2 model.**"
      ],
      "metadata": {
        "id": "5stjSzpm6Cvc"
      },
      "id": "5stjSzpm6Cvc"
    },
    {
      "cell_type": "code",
      "source": [
        "# (b)\n",
        "# Embed the Benchmark solution text using the Hugging Face embeddings model and reshape it into a 2D array\n",
        "\n",
        "# YOUR CODE HERE\n",
        "\n",
        "# Generate a 2D array representation of the embedded response by using Hugging Face embeddings (specified under Exercise-5)\n",
        "\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "_jT8qYssyHmH"
      },
      "id": "_jT8qYssyHmH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the cosine similarity between the benchmark solution embeddings and the RAG response embeddings\n",
        "# to evaluate their similarity. Print the Cosine Similarity value.\n",
        "\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "_Kreb5M8ycpT"
      },
      "id": "_Kreb5M8ycpT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fill in the following blank with the help of the above results [achieved in Exercise-8(b)]\n",
        "\n",
        "**So considering the Benchmark_solution BMS, we can observe from the above value, that the Cosine Similarity is ______% by using RAG Architecture with Microsoft Phi-2 model.**"
      ],
      "metadata": {
        "id": "PjT0uzJHypCC"
      },
      "id": "PjT0uzJHypCC"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Phase-III:** Performing RAG using HuggingFace Retrieval Chain For 5 different Embedding models and FAISS Vector Store\n",
        "\n",
        "We will use CSV Dataset for this phase.\n",
        "\n",
        "<br><br>\n",
        "<center>\n",
        "<img src=\" https://cdn.exec.talentsprint.com/static/cds/content/varying_embeddings-4.png\" height = 600 width= 1600 px/>\n",
        "</center>\n",
        "<br><br>"
      ],
      "metadata": {
        "id": "Ls6NTwqIDP7T"
      },
      "id": "Ls6NTwqIDP7T"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Load Data (CSV Dataset)"
      ],
      "metadata": {
        "id": "dyRf60DgFU0l"
      },
      "id": "dyRf60DgFU0l"
    },
    {
      "cell_type": "code",
      "source": [
        "loader = CSVLoader(file_path='/content/demo_faqs.csv', source_column=\"prompt\",encoding='latin-1')\n",
        "\n",
        "# Store the loaded data in the 'data' variable\n",
        "data = loader.load()\n",
        "documents_csv = data"
      ],
      "metadata": {
        "id": "wRLA-EQ-FGpp"
      },
      "id": "wRLA-EQ-FGpp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Using 5 different HuggungFace Embedding Models"
      ],
      "metadata": {
        "id": "FIl3EmPhXHK9"
      },
      "id": "FIl3EmPhXHK9"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define embedding model-1\n",
        "embed_model_1 = HuggingFaceEmbeddings(model_name='BAAI/bge-small-en-v1.5')\n",
        "\n",
        "# Define embedding model-2\n",
        "embed_model_2 = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "# Define embedding model-3\n",
        "embed_model_3 = HuggingFaceEmbeddings(model_name='sentence-transformers/paraphrase-MiniLM-L12-v2')\n",
        "\n",
        "# Define embedding model-4\n",
        "embed_model_4 = HuggingFaceEmbeddings(model_name='sentence-transformers/all-distilroberta-v1')\n",
        "\n",
        "# Define embedding model-5\n",
        "embed_model_5 = HuggingFaceEmbeddings(model_name='sentence-transformers/multi-qa-MiniLM-L6-cos-v1')"
      ],
      "metadata": {
        "id": "XZVk85YQXZDN"
      },
      "id": "XZVk85YQXZDN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 Vector store using FAISS\n",
        "\n",
        "Facebook AI Similarity Search (FAISS) is a library for efficient similarity search and clustering of dense vectors. It contains algorithms that search in sets of vectors of any size, up to ones that possibly do not fit in RAM. It also contains supporting code for evaluation and parameter tuning.\n",
        "For further details, please refer to the [link](https://faiss.ai/)\n",
        "\n",
        "How to use functionality related to the FAISS vector database?\n",
        "\n",
        "In the following code cell, we will show functionality specific to this integration. After going through, it may be useful to explore relevant to learn how to use this vectorstore as part of a larger chain.\n",
        "\n",
        "**Exercise-9:** Create a FAISS vector database using Hugging Face Embeddings model 'BAAI/bge-small-en-v1.5'. Then retrieve relevant answers for a query. Use 'get_relevant_documents()'. **(0.5 point)**"
      ],
      "metadata": {
        "id": "uf7zGpKPlq5q"
      },
      "id": "uf7zGpKPlq5q"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a FAISS vector database from 'data' (loaded under section 3.1)\n",
        "# using the specified embedding model 'BAAI/bge-small-en-v1.5'\n",
        "\n",
        "# YOUR CODE HERE\n",
        "\n",
        "# Create a retriever for querying the vector database with a specified score threshold = 0.7 to filter relevant results\n",
        "# Store it in a variable 'h_retriever_1'\n",
        "\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "ljLHwiZFlzs_"
      },
      "id": "ljLHwiZFlzs_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above code cell, The provided code snippet sets up a FAISS (Facebook AI Similarity Search) vector database to store document embeddings and enables querying this database using a retriever with a specific score threshold.\n",
        "\n",
        "- **FAISS.from_documents(...)**: This method initializes a FAISS vector database using a list of documents and a pre-defined embedding model.\n",
        "- **h_vectordb.as_retriever(...)**: This method converts the FAISS vector database into a retriever object that can be queried using natural language or embedded queries."
      ],
      "metadata": {
        "id": "qZDaBYVnulVo"
      },
      "id": "qZDaBYVnulVo"
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve relevant documents from the vector database (achieved in Exercise-9)\n",
        "# based on a specific query, such as \"How about job placement support?\"\n",
        "\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "Czt6whRgurQW"
      },
      "id": "Czt6whRgurQW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above code cell,\n",
        "\n",
        "- **h_retriever.get_relevant_documents(...)**: This method queries the retriever (which is linked to the FAISS vector database) with a given text query."
      ],
      "metadata": {
        "id": "kQtPWweLvAcu"
      },
      "id": "kQtPWweLvAcu"
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see above, the retriever that was created using FAISS and Hugging Face Embedding is now capable of pulling relavant documents from the original CSV file knowledge store. This is very powerful and it will help us further in this project."
      ],
      "metadata": {
        "id": "0rWlknGZvHwE"
      },
      "id": "0rWlknGZvHwE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise-10:** Create a FAISS vector database using Embeddings model 'sentence-transformers/all-MiniLM-L6-v2'. Then retrieve relevant answers for a query. Use 'get_relevant_documents()'. **(0.5 point)**"
      ],
      "metadata": {
        "id": "tixqezfDZ3hj"
      },
      "id": "tixqezfDZ3hj"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a FAISS vector database from 'data' using the specified embedding model 'sentence-transformers/all-MiniLM-L6-v2'\n",
        "\n",
        "# YOUR CODE HERE\n",
        "\n",
        "# Create a retriever for querying the vector database with a specified score threshold = 0.7\n",
        "# Store it in a variable 'h_retriever_2'\n",
        "\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "aFB0HAh9Y4Pu"
      },
      "id": "aFB0HAh9Y4Pu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve relevant documents from the vector database (achieved in Exercise-10)\n",
        "# based on a specific query \"How about job placement support?\"\n",
        "\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "BwO6HE5gY-cw"
      },
      "id": "BwO6HE5gY-cw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise-11:** Create a FAISS vector database using Embeddings model 'sentence-transformers/paraphrase-MiniLM-L12-v2'. Then retrieve relevant answers for a query. Use 'get_relevant_documents()'. **(0.5 point)**"
      ],
      "metadata": {
        "id": "89X3iUpIaUSU"
      },
      "id": "89X3iUpIaUSU"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a FAISS vector database from 'data' using the specified embedding model 'sentence-transformers/paraphrase-MiniLM-L12-v2'\n",
        "\n",
        "# YOUR CODE HERE\n",
        "\n",
        "# Create a retriever for querying the vector database with a specified score threshold = 0.7\n",
        "# Store it in a variable 'h_retriever_3'\n",
        "\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "X1w_fbkCakGG"
      },
      "id": "X1w_fbkCakGG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve relevant documents from the vector database (achieved in Exercise-11)\n",
        "# based on the specific query \"How about job placement support?\"\n",
        "\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "nOdlYZQTaw7-"
      },
      "id": "nOdlYZQTaw7-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise-12:** Create a FAISS vector database using Embeddings model 'sentence-transformers/all-distilroberta-v1'. Then retrieve relevant answers for a query. Use 'get_relevant_documents()'. **(0.5 point)**"
      ],
      "metadata": {
        "id": "2ScvX_Dza9Iu"
      },
      "id": "2ScvX_Dza9Iu"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a FAISS vector database from 'data' using the specified embedding model 'sentence-transformers/all-distilroberta-v1'\n",
        "\n",
        "# YOUR CODE HERE\n",
        "\n",
        "# Create a retriever for querying the vector database based on a specific score threshold = 0.7\n",
        "# Store it in a variable 'h_retriever_4'\n",
        "\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "q-CZ55rjbKGk"
      },
      "id": "q-CZ55rjbKGk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve relevant documents from the vector database (achieved in Exercise-12)\n",
        "# based on the specific query \"How about job placement support?\"\n",
        "\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "CJf10AnLbfMn"
      },
      "id": "CJf10AnLbfMn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise-13:** Create a FAISS vector database using Embeddings model 'sentence-transformers/multi-qa-MiniLM-L6-cos-v1'. Then retrieve relevant answers for a query. Use 'get_relevant_documents()'. **(0.5 point)**"
      ],
      "metadata": {
        "id": "ZORYEVYsbmfr"
      },
      "id": "ZORYEVYsbmfr"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a FAISS vector database from 'data' using a specified embedding model 'sentence-transformers/multi-qa-MiniLM-L6-cos-v1'\n",
        "\n",
        "# YOUR CODE HERE\n",
        "\n",
        "# Create a retriever for querying the vector database based on a specific score threshold = 0.7\n",
        "# Store it in a variable 'h_retriever_5'\n",
        "\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "HAuiGfAFbvrk"
      },
      "id": "HAuiGfAFbvrk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve relevant documents from the vector database (achieved in Exercise-13)\n",
        "# based on the specific query \"How about job placement support?\"\n",
        "\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "Sb1WgfD9b6Xh"
      },
      "id": "Sb1WgfD9b6Xh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.4 Create RetrievalQA chain with FAISS Vectore Store & Hugging Face 🚀\n",
        "\n",
        "**Exercise-14:** Create RetrievalQA chains for 5 different HuggungFace Embedding Models. Use llm model zephyr_7b_beta and use PromptTemplate to get PROMPT. Then use 'RetrievalQA.from_chain_type()' for getting the 5 Hugging Face RetrievalQA chains. **(0.5 point)**"
      ],
      "metadata": {
        "id": "hmgfg_bnwYQZ"
      },
      "id": "hmgfg_bnwYQZ"
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = \"\"\"Given the following context and a question, generate an answer based on this context only.\n",
        "In the answer try to provide as much text as possible from \"response\" section in the source document context without making much changes.\n",
        "If the answer is not found in the context, kindly state \"I don't know.\" Don't try to make up an answer.\n",
        "\n",
        "CONTEXT: {context}\n",
        "\n",
        "QUESTION: {question}\"\"\"\n",
        "\n",
        "\n",
        "PROMPT = PromptTemplate(input_variables=[\"context\", \"question\"], template=prompt_template)\n",
        "chain_type_kwargs = {\"prompt\": PROMPT}\n",
        "\n",
        "# create a RetrievalQA chain in LangChain using the 'zephyr_7b_beta_HFE_llm' language model (derived in Exercise-3),\n",
        "# specifying the chain_type as \"stuff\",\n",
        "# integrating the 'h_retriever_1' (refer Exercise-9), and\n",
        "# ensuring that the source documents are returned alongside the answers to the user's queries.\n",
        "\n",
        "# YOUR CODE HERE\n",
        "\n",
        "# create a RetrievalQA chain in LangChain using the 'zephyr_7b_beta_HFE_llm' language model (derived in Exercise-3),\n",
        "# specifying the chain_type as \"stuff\",\n",
        "# integrating the 'h_retriever_2' (refer Exercise-10), and\n",
        "# ensuring that the source documents are returned alongside the answers to the user's queries.\n",
        "\n",
        "# YOUR CODE HERE\n",
        "\n",
        "# create a RetrievalQA chain in LangChain using the 'zephyr_7b_beta_HFE_llm' language model (derived in Exercise-3),\n",
        "# specifying the chain_type as \"stuff\",\n",
        "# integrating the 'h_retriever_3' (refer Exercise-11), and\n",
        "# ensuring that the source documents are returned alongside the answers to the user's queries.\n",
        "\n",
        "# YOUR CODE HERE\n",
        "\n",
        "# create a RetrievalQA chain in LangChain using the 'zephyr_7b_beta_HFE_llm' language model (derived in Exercise-3),\n",
        "# specifying the chain_type as \"stuff\",\n",
        "# integrating the 'h_retriever_4' (refer Exercise-12), and\n",
        "# ensuring that the source documents are returned alongside the answers to the user's queries.\n",
        "\n",
        "# YOUR CODE HERE\n",
        "\n",
        "# create a RetrievalQA chain in LangChain using the 'zephyr_7b_beta_HFE_llm' language model (derived in Exercise-3),\n",
        "# specifying the chain_type as \"stuff\",\n",
        "# integrating the 'h_retriever_5' (refer Exercise-13), and\n",
        "# ensuring that the source documents are returned alongside the answers to the user's queries.\n",
        "\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "LGmxitlrwhm3"
      },
      "id": "LGmxitlrwhm3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above code cell,  The code snippet sets up a RetrievalQA chain using a custom prompt template with a Hugging Face language model and a retriever.\n",
        "\n",
        "- **PromptTemplate(...)**: Initializes a PromptTemplate object from the langchain.prompts module.\n",
        "- **template=prompt_template**: Specifies the template string created above.\n",
        "- **input_variables=[\"context\", \"question\"]**: Defines the placeholders in the template that will be replaced by actual context and question values during the query.\n",
        "- **chain_type_kwargs**: This dictionary contains the prompt key with the PROMPT object, which will be used to format the queries sent to the language model.\n",
        "- **RetrievalQA.from_chain_type(...)**: Initializes a RetrievalQA chain.\n",
        "- **llm=h_llm**: Specifies the language model (h_llm) to be used for generating answers.\n",
        "- **chain_type=\"stuff\"**: Defines the type of chain. In this case, \"stuff\" is a placeholder that can be replaced with other chain types depending on the use case.\n",
        "- **retriever=h_retriever**: Provides the retriever (h_retriever) that will be used to fetch relevant context from the vector database.\n",
        "- **input_key=\"query\"**: Indicates the key used to pass the query to the chain.\n",
        "return_source_documents=True: Ensures that the source documents used to generate the answer are returned along with the answer.\n",
        "- **chain_type_kwargs=chain_type_kwargs**: Passes additional keyword arguments (including the prompt template) to the chain."
      ],
      "metadata": {
        "id": "iqj8LeDGxYlx"
      },
      "id": "iqj8LeDGxYlx"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.5 Let's ask some questions to FAISS based Hugging Face RetrievalQA chain\n",
        "\n",
        "**Exercise-15:** Execute a retrieval-based QA query for the question: 'Do you provide job assistance and also do you provide job guarantee?' using each of the 5 ReyrievalQA chains as achieved in Exercise-14. **(0.5 point)**"
      ],
      "metadata": {
        "id": "hUfOxRlZxfSY"
      },
      "id": "hUfOxRlZxfSY"
    },
    {
      "cell_type": "code",
      "source": [
        "Q1 = 'Do you provide job assistance and also do you provide job gurantee?'\n",
        "\n",
        "# Invoke the RetrievalQA chain 'h_retriever_1' (refer Exercise-9) with the specific query Q1 to retrieve the corresponding answer and any relevant documents\n",
        "# The output will be a dictionary. Consider it as 'h_retrieval_QA1'\n",
        "\n",
        "# YOUR CODE HERE\n",
        "\n",
        "# Get the list of keys in the dictionary 'h_retrieval_QA1'\n",
        "\n",
        "# YOUR CODE HERE\n",
        "\n",
        "# Access the value using the key's index and store the value in 'h_result_value1'\n",
        "\n",
        "# YOUR CODE HERE  (use 1 as the index of 'result' key)\n",
        "######################################################\n",
        "# Invoke the RetrievalQA chain 'h_retriever_2' (refer Exercise-10) with the specific query Q1 to retrieve the corresponding answer and any relevant documents\n",
        "# The output will be a dictionary. Consider it as 'h_retrieval_QA2'\n",
        "\n",
        "# YOUR CODE HERE\n",
        "\n",
        "# Get the list of keys in the dictionary 'h_retrieval_QA2'\n",
        "\n",
        "# YOUR CODE HERE\n",
        "\n",
        "# Access the value using the key's index and store the value in 'h_result_value2'\n",
        "\n",
        "# YOUR CODE HERE  (use 1 as the index of 'result' key)\n",
        "######################################################\n",
        "# Invoke the RetrievalQA chain 'h_retriever_3' (refer Exercise-11) with the specific query Q1 to retrieve the corresponding answer and any relevant documents\n",
        "# The output will be a dictionary. Consider it as 'h_retrieval_QA3'\n",
        "\n",
        "# YOUR CODE HERE\n",
        "\n",
        "# Get the list of keys in the dictionary 'h_retrieval_QA3'\n",
        "\n",
        "# YOUR CODE HERE\n",
        "\n",
        "# Access the value using the key's index and store the value in 'h_result_value3'\n",
        "\n",
        "# YOUR CODE HERE  (use 1 as the index of 'result' key)\n",
        "######################################################\n",
        "# Invoke the RetrievalQA chain 'h_retriever_4' (refer Exercise-12) with the specific query Q1 to retrieve the corresponding answer and any relevant documents\n",
        "# The output will be a dictionary. Consider it as 'h_retrieval_QA4'\n",
        "\n",
        "# YOUR CODE HERE\n",
        "\n",
        "# Get the list of keys in the dictionary 'h_retrieval_QA4'\n",
        "\n",
        "# YOUR CODE HERE\n",
        "\n",
        "# Access the value using the key's index and store the value in 'h_result_value4'\n",
        "\n",
        "# YOUR CODE HERE  (use 1 as the index of 'result' key)\n",
        "######################################################\n",
        "# Invoke the RetrievalQA chain 'h_retriever_5' (refer Exercise-13) with the specific query Q1 to retrieve the corresponding answer and any relevant documents\n",
        "# The output will be a dictionary. Consider it as 'h_retrieval_QA5'\n",
        "\n",
        "# YOUR CODE HERE\n",
        "\n",
        "# Get the list of keys in the dictionary 'h_retrieval_QA5'\n",
        "\n",
        "# YOUR CODE HERE\n",
        "\n",
        "# Access the value using the key's index and store the value in 'h_result_value5'\n",
        "\n",
        "# YOUR CODE HERE  (use 1 as the index of 'result' key)"
      ],
      "metadata": {
        "id": "RHH8wCm8xleh"
      },
      "id": "RHH8wCm8xleh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fromthe above results, you will be able to see, that the answer of question comes from two different FAQs within the Codebasics FAQ csv file and it is able to pull those questions and merge them nicely.**"
      ],
      "metadata": {
        "id": "LOI1SQakxr3_"
      },
      "id": "LOI1SQakxr3_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.6 Comparison: 5 different embedding models performance (for FAISS Vectore Store)\n",
        "\n",
        "**Exercise-16:** Compare the RetrievalQA performance among all 5 different Embedding Models using Cosine Similarity.\n",
        "\n",
        "Use the embeddig models achieved under section 3.2. **(0.5 point)**\n",
        "\n",
        "- **(a)** Consider the reference Question: 'Do you provide job assistance and also do you provide job guarantee?'. Compute Cosine Similarity.\n",
        "\n",
        "- **(b)** Consider the Benchmark_response: 'Yes, We help you with resume and interview preparation along with that we help you in building online credibility, and based on requirements we refer candidates to potential recruiters.' Compute Cosine Similarity."
      ],
      "metadata": {
        "id": "FVuehG4whb_g"
      },
      "id": "FVuehG4whb_g"
    },
    {
      "cell_type": "code",
      "source": [
        "# Refer to the 5 different HuggungFace Embedding Models as already created under section 3.2\n",
        "# Put those 5 embed models as below.\n",
        "h_embeddings1 = embed_model_1\n",
        "h_embeddings2 = embed_model_2\n",
        "h_embeddings3 = embed_model_3\n",
        "h_embeddings4 = embed_model_4\n",
        "h_embeddings5 = embed_model_5\n",
        "\n",
        "Benchmark_response = \"\"\"Yes, We help you with resume and interview preparation along with that we help you in building online credibility,\n",
        "and based on requirements we refer candidates to potential recruiters.\"\"\"\n",
        "\n",
        "BMR = Benchmark_response\n",
        "\n",
        "# Reshape the output of embedding the query (Q1 as given in Exercose-15) using the 1st embedding model (h_embeddings1) into a 2D array.\n",
        "# YOUR CODE HERE\n",
        "# Convert the output of embedding the BMR (mentioned above) using the 1st embedding model (h_embeddings1) into a 2D array.\n",
        "# YOUR CODE HERE\n",
        "# Reshape the output of embedding the h_result_value1 (which was created in Exercise-15) into a 2D array using h_embeddings1.\n",
        "# YOUR CODE HERE\n",
        "\n",
        "# Reshape the output of embedding the query (Q1 as given in Exercose-15) using the 2nd embedding model (h_embeddings2) into a 2D array.\n",
        "# YOUR CODE HERE\n",
        "# Convert the output of embedding the BMR (mentioned above) using the 2nd embedding model (h_embeddings2) into a 2D array.\n",
        "# YOUR CODE HERE\n",
        "# Reshape the output of embedding the h_result_value2 (which was created in Exercise-15) into a 2D array using h_embeddings2.\n",
        "# YOUR CODE HERE\n",
        "\n",
        "# Reshape the output of embedding the query (Q1 as given in Exercose-15) using the 3rd embedding model (h_embeddings3) into a 2D array.\n",
        "# YOUR CODE HERE\n",
        "# Convert the output of embedding the BMR (mentioned above) using the 3rd embedding model (h_embeddings3) into a 2D array.\n",
        "# YOUR CODE HERE\n",
        "# Reshape the output of embedding the h_result_value3 (which was created in Exercise-15) into a 2D array using h_embeddings3.\n",
        "# YOUR CODE HERE\n",
        "\n",
        "# Reshape the output of embedding the query (Q1 as given in Exercose-15) using the 4th embedding model (h_embeddings4) into a 2D array.\n",
        "# YOUR CODE HERE\n",
        "# Convert the output of embedding the BMR (mentioned above) using the 4th embedding model (h_embeddings4) into a 2D array.\n",
        "# YOUR CODE HERE\n",
        "# Reshape the output of embedding the h_result_value4 (which was created in Exercise-15) into a 2D array using h_embeddings4.\n",
        "# YOUR CODE HERE\n",
        "\n",
        "# Reshape the output of embedding the query (Q1 as given in Exercose-15) using the 5th embedding model (h_embeddings5) into a 2D array.\n",
        "# YOUR CODE HERE\n",
        "# Convert the output of embedding the BMR (mentioned above) using the 4th embedding model (h_embeddings5) into a 2D array.\n",
        "# YOUR CODE HERE\n",
        "# Reshape the output of embedding the h_result_value5 (which was created in Exercise-15) into a 2D array using h_embeddings5.\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "nZNw9BCNjLGM"
      },
      "id": "nZNw9BCNjLGM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (a)\n",
        "# Calculate and display the cosine similarity between\n",
        "# (i) Q1 (as given in Exercose-15) and h_result_value1, (ii) Q1 (as given in Exercose-15) and h_result_value2,\n",
        "# (iii) Q1 (as given in Exercose-15) and h_result_value3, (iv) Q1 (as given in Exercose-15) and h_result_value4, and\n",
        "# (v) Q1 (as given in Exercose-15) and h_result_value5\n",
        "\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "5JZxzzezmQaU"
      },
      "id": "5JZxzzezmQaU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fill in the following blanks.\n",
        "\n",
        "**So, by considering the reference query Q1(as given in Exercose-15), as we can observe from the above result, that the highest Cosine Similarity (______%) is achieved by using the HuggingFace embedding model '_______________________'.**\n",
        "\n",
        "**So after the below code cell, use the corresponding RetrievalQA chain (i.e., ________) which is the best out of 5 RetrievalQA chains) to ask following queries and to get responses.**"
      ],
      "metadata": {
        "id": "QWGD__2rnYPz"
      },
      "id": "QWGD__2rnYPz"
    },
    {
      "cell_type": "code",
      "source": [
        "# (b)\n",
        "# Compute and print the cosine similarity between the benchmark reference embedding (BMR) and\n",
        "# multiple result embeddings, indicating their similarity scores\n",
        "\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "ejEkBm9u3vc_"
      },
      "id": "ejEkBm9u3vc_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fill in the following blanks.\n",
        "\n",
        "**So, by considering the Benchmark_response BMR (mentioned in Exercise-16), as we can observe from the above result, that the highest Cosine Similarity (______%) is achieved by using the embedding model ___________________________.**"
      ],
      "metadata": {
        "id": "doWpjEDh4OfM"
      },
      "id": "doWpjEDh4OfM"
    },
    {
      "cell_type": "code",
      "source": [
        "# Ask Question \"Do you guys provide internship and also do you offer EMI payments?\"\"\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "G2WqFoJrxyzY"
      },
      "id": "G2WqFoJrxyzY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ask Question \"do you have javascript course?\"\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "Z-63uWBdx1T_"
      },
      "id": "Z-63uWBdx1T_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ask Question \"Do you have plans to launch blockchain course in future?\"\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "S0VJ48Olx4Co"
      },
      "id": "S0VJ48Olx4Co",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ask Question \"should I learn power bi or tableau?\"\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "KryCvsdhx6mb"
      },
      "id": "KryCvsdhx6mb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ask Question \"I've a MAC computer. Can I use powerbi on it?\"\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "MP_ZrpkHx9LY"
      },
      "id": "MP_ZrpkHx9LY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ask Question \"I don't see power pivot. how can I enable it?\"\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "t3JvaRNTx_tH"
      },
      "id": "t3JvaRNTx_tH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ask Question \"What is the price of your machine learning course?\"\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "774_F3gRyCKQ"
      },
      "id": "774_F3gRyCKQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Phase-IV:** Performing RAG using HuggingFace Retrieval Chain For Fixed Embedding model and Chromadb Vector Store\n",
        "\n",
        "In this Phase-IV, the vector store is changed from FAISS to Chromadb\n",
        "\n",
        "<br><br>\n",
        "<center>\n",
        "<img src=\" https://cdn.exec.talentsprint.com/static/cds/content/varying_vector_stores-5.png\" height = 600 width= 1600 px/>\n",
        "</center>\n",
        "<br><br>"
      ],
      "metadata": {
        "id": "3r6qc23JnHZe"
      },
      "id": "3r6qc23JnHZe"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 Vector store using Chromadb\n",
        "\n",
        "##### For vector database we can use chromadb as shown below. During the experimentation, we found Hugging Face Embeddings and FAISS to be appropriate for our use case. Let's see the retrieval performance using Chromadb in the following code cell.\n",
        "\n",
        "**Exercise-17:** Create a Chroma vector database. Use the above achieved best Hugging Face Embeddings model 'BAAI/bge-small-en-v1.5'. Then retrieve relevant answers for a query. Use 'get_relevant_documents()' **(0.5 point)**"
      ],
      "metadata": {
        "id": "I5NU2wZhqpWu"
      },
      "id": "I5NU2wZhqpWu"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Chroma vector database from documents using embed_model_1 that was created under section 3.2 and\n",
        "# persist it to a specified directory\n",
        "\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "vf69UpUXrHIP"
      },
      "id": "vf69UpUXrHIP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a retriever for querying the vector database derived through Chroma with a score_threshold = 0.7\n",
        "\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "ipLGSB14rOjj"
      },
      "id": "ipLGSB14rOjj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Retrieve relevant documents related to the query \"how about job placement support?\"\n",
        "\n",
        " # YOUR CODE HERE"
      ],
      "metadata": {
        "id": "co-yOQsSrWQK"
      },
      "id": "co-yOQsSrWQK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above code cell,\n",
        "\n",
        "- **Chroma.from_documents(...)**: This method initializes a Chroma vector database using a list of documents, an embedding model, and a directory to persist the database.\n",
        "- **g_vectordb.as_retriever(...)**: This method converts the Chroma vector database instance (g_vectordb) into a retriever object that can be used to perform queries.\n",
        "- **g_retriever.get_relevant_documents(...)**: This method queries the retriever object (g_retriever) with the given text query."
      ],
      "metadata": {
        "id": "Ka1skkNwrg42"
      },
      "id": "Ka1skkNwrg42"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2 Create RetrievalQA chain with Chromadb Vectore Store & Hugging Face 🚀\n",
        "\n",
        "**Exercise-18:** Now we will use the achieved best embedding model as evaluated in Exercise-16 (i.e., HuggingFace embedding model 'BAAI/bge-small-en-v1.5') to see if there is any impact in RetrievalQA chain's performance if the Vector Store is changed from FAISS to Chromadb. Create RetrievalQA chain with Chromadb Vectore Store. Use PromptTemplate to get PROMPT. Then use 'RetrievalQA.from_chain_type()' for getting the Chromadb Vectore Store based RetrievalQA chain. **(0.5 point)**"
      ],
      "metadata": {
        "id": "Qw2kodBOJSWp"
      },
      "id": "Qw2kodBOJSWp"
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = \"\"\"Given the following context and a question, generate an answer based on this context only.\n",
        "In the answer try to provide as much text as possible from \"response\" section in the source document context without making much changes.\n",
        "If the answer is not found in the context, kindly state \"I don't know.\" Don't try to make up an answer.\n",
        "\n",
        "CONTEXT: {context}\n",
        "\n",
        "QUESTION: {question}\"\"\"\n",
        "\n",
        "\n",
        "PROMPT = PromptTemplate(\n",
        "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "chain_type_kwargs = {\"prompt\": PROMPT}\n",
        "\n",
        "# Create a RetrievalQA chain using the 'zephyr_7b_beta_HFE_llm' model (derived in Exercise-3) and\n",
        "# the retriever created in Exercise-17, while returning source documents and customizing chain type arguments\n",
        "\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "CFPOVQL-KHDa"
      },
      "id": "CFPOVQL-KHDa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above code cell, The code snippet sets up a RetrievalQA chain using a custom prompt template with a Google PaLM language model and a retriever.\n",
        "\n",
        "- **PromptTemplate(...)**: Initializes a PromptTemplate object from the langchain.prompts module.\n",
        "- **template=prompt_template**: Specifies the template string that defines how queries should be formatted.\n",
        "- **input_variables=[\"context\", \"question\"]**: Lists the placeholders in the template that will be replaced by actual values for context and question.\n",
        "chain_type_kwargs: A dictionary that includes the prompt template used to format the queries.\n",
        "- **RetrievalQA.from_chain_type(...)**: Initializes a RetrievalQA chain.\n",
        "- **llm=g_llm**: Specifies the Google PaLM language model (g_llm) used for generating answers.\n",
        "- **chain_type=\"stuff\"**: Defines the type of chain. \"stuff\" can be replaced with other chain types as needed.\n",
        "- **retriever=g_retriever**: The retriever (g_retriever) used to fetch relevant documents from the vector database.\n",
        "- **input_key=\"query\"**: Indicates the key used for passing the query to the chain.\n",
        "- **return_source_documents=True**: Ensures that the documents used to generate the answer are returned along with the answer.\n",
        "- **chain_type_kwargs=chain_type_kwargs**: Passes additional keyword arguments, including the prompt template, to the chain."
      ],
      "metadata": {
        "id": "uu-CsmY_KiLm"
      },
      "id": "uu-CsmY_KiLm"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3 Let's ask some questions to Chromadb based HuggingFace retrieval QA chain\n",
        "\n",
        "**Exercise-19:** By using the Chromadb Vector Store based Retrieval QA chain (achieved in Exercise-18), execute a retrieval-based QA query for the question: 'Do you provide job assistance and also do you provide job guarantee?'. **(0.5 point)**"
      ],
      "metadata": {
        "id": "Z1SfPyW3KqFO"
      },
      "id": "Z1SfPyW3KqFO"
    },
    {
      "cell_type": "code",
      "source": [
        "Q1 = 'Do you provide job assistance and also do you provide job gurantee?'\n",
        "# Using the Chromadb Vector Store based Retrieval QA chain (achieved in Exercise-18),\n",
        "# execute a retrieval-based QA query for the question Q1 (mentioned above). The output will be a dictionary.\n",
        "# Name it as 'g_retrieval_QA1'\n",
        "\n",
        "# YOUR CODE HERE\n",
        "\n",
        "# Get the list of keys in the dictionary 'g_retrieval_QA1'\n",
        "\n",
        "# YOUR CODE HERE\n",
        "\n",
        "# Access the value using the key's index. Store the value in a variable 'g_result_value1'\n",
        "# YOUR CODE HERE  (Use 1 as the index of 'result' key)"
      ],
      "metadata": {
        "id": "iwIB6EVpLb8Z"
      },
      "id": "iwIB6EVpLb8Z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.4 Comparison: Is there any impact?\n",
        "- keeping the llm and embedding model unchanged but only changing the Vector Store from FAISS to Chromadb\n",
        "\n",
        "**Exercise-20:** Using Cosine Similarity, measure the RetrievalQA performance of the Chromadb based RetrievalQA chain as achieved in Exercise-18. Use the best embeddig model as evaluated in Exercise-16 (i.e., HuggingFace embedding model 'BAAI/bge-small-en-v1.5').\n",
        "\n",
        "Consider the reference Question: 'Do you provide job assistance and also do you provide job guarantee?'. **(0.5 point)**"
      ],
      "metadata": {
        "id": "KaxvDh6qs_oL"
      },
      "id": "KaxvDh6qs_oL"
    },
    {
      "cell_type": "code",
      "source": [
        "# Using HuggingFaceEmbeddings 'BAAI/bge-small-en-v1.5'\n",
        "embeddings = embed_model_1\n",
        "\n",
        "# Convert a query into an embedding array using the embed_query() function and reshape it to a 2D array for Q1\n",
        "\n",
        "# YOUR CODE HERE\n",
        "\n",
        "# Convert 'g_result_value1' created in Exercise-19 into an embedding array using the embed_query() function and\n",
        "# reshape it to a 2D array\n",
        "\n",
        "# YOUR COE# HERE"
      ],
      "metadata": {
        "id": "LZ91qia2u2c9"
      },
      "id": "LZ91qia2u2c9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute Cosine Similarity between Q1 (mentioned in Exercise-19) and\n",
        "# Chromadb based 'g_result_value1' (as derived in Exercise-19) and store the result in a variable 'cosine_sim_Chromadb'?\n",
        "\n",
        "# YOUR CODE HERE\n",
        "\n",
        "#cosine_sim_Chromadb = cosine_similarity(Q1_e, g_e1)[0][0]\n",
        "\n",
        "#print(f\"Cosine Similarity between Q1 and FAISS based h_result_value1: {cosine_sim_FAISS}\")\n",
        "print(f\"Cosine Similarity between Q1 and Chromadb based g_result_value1: {cosine_sim_Chromadb}\")"
      ],
      "metadata": {
        "id": "ZxPG-x8jvPyc"
      },
      "id": "ZxPG-x8jvPyc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Cosine Similarity between Q1 and h_result_value1: {cosine_sim_1}\") # consider 'cosine_sim_1' from Exercise-16(a)\n",
        "print(f\"Difference in Cosine Similarity between FAISS and Chromadb: {cosine_sim_1 - cosine_sim_Chromadb}\")\n",
        "print(f\"Percentage Difference in Cosine Similarity between FAISS and Chromadb: {(cosine_sim_1 - cosine_sim_Chromadb)*100}%\")"
      ],
      "metadata": {
        "id": "eOivgmHIwQw9"
      },
      "id": "eOivgmHIwQw9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fill in the blank.\n",
        "\n",
        "**Hence, from the above result we can observe that in RAG performance, there is ______% difference (i.e., very low difference) in Cosine Similarity between FAISS and Chromadb based retrieval chain if the llm and embedding model are remained unchanged. So, there is very less impact of changing the Vector Store, if the llm and embedinng model remain same.**"
      ],
      "metadata": {
        "id": "12nw5P-5w-dv"
      },
      "id": "12nw5P-5w-dv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Optional Task:** Execute the below code cells to test the RAG performance with the following queries. Use Chromadb based RetrievalQA chain as obtained in Exercise-18."
      ],
      "metadata": {
        "id": "UtvA_G-VyaND"
      },
      "id": "UtvA_G-VyaND"
    },
    {
      "cell_type": "code",
      "source": [
        "# EXECUTE queries \"do you have javascript course?\"\"\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "TJE7fBxfLjdR"
      },
      "id": "TJE7fBxfLjdR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EXECUTE queries \"Do you have plans to launch blockchain course in future?\"\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "z36Slr3HLnhP"
      },
      "id": "z36Slr3HLnhP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EXECUTE queries \"should I learn power bi or tableau?\"\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "5tmPCGkbLqzx"
      },
      "id": "5tmPCGkbLqzx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EXECUTE queries \"I've a MAC computer. Can I use powerbi on it?\"\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "IIDDyInyLuDR"
      },
      "id": "IIDDyInyLuDR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EXECUTE queries \"I don't see power pivot. how can I enable it?\"\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "6IR1zM2iLzTi"
      },
      "id": "6IR1zM2iLzTi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EXECUTE queries \"What is the price of your machine learning course?\"\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "epIJt6kXL2mc"
      },
      "id": "epIJt6kXL2mc",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "67d1742d6c1d4907bdcc16a8f1a7fcb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ce6d7f8482c249949fc81c29092b947a",
              "IPY_MODEL_1739b8125a284b5db23f6e5bba352846",
              "IPY_MODEL_bd3b14096fb548c9be24136ae97b06a7"
            ],
            "layout": "IPY_MODEL_a98a47af5156429389d3bc9944d78ab0"
          }
        },
        "ce6d7f8482c249949fc81c29092b947a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bdd494cfafe6456eb6705a860a65a33e",
            "placeholder": "​",
            "style": "IPY_MODEL_cc6c72d799a74449953843af428cc39b",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "1739b8125a284b5db23f6e5bba352846": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c0d50216ea8e497cbd18f664e3b7ff07",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7ade35d08ebc46a08d3f671e4faab597",
            "value": 2
          }
        },
        "bd3b14096fb548c9be24136ae97b06a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_057c6b7f0ae048dbb8d8b5f6483a8c7e",
            "placeholder": "​",
            "style": "IPY_MODEL_a711137a125f41ada1c7c91b58095c89",
            "value": " 2/2 [00:20&lt;00:00,  8.94s/it]"
          }
        },
        "a98a47af5156429389d3bc9944d78ab0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bdd494cfafe6456eb6705a860a65a33e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc6c72d799a74449953843af428cc39b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c0d50216ea8e497cbd18f664e3b7ff07": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ade35d08ebc46a08d3f671e4faab597": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "057c6b7f0ae048dbb8d8b5f6483a8c7e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a711137a125f41ada1c7c91b58095c89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}